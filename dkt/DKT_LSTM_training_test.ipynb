{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3T3FI2rz09Xg"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**: KDD Bridge to Algebra 2006-2007\n",
        "\n",
        "**Source Code**: Our implementation of the LSTM version of Deep Knowledge Tracing (DKT)\n",
        "\n",
        "https://github.com/shinyflight/Deep-Knowledge-Tracing"
      ],
      "metadata": {
        "id": "rsniyGQXe_0U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-x0MQeh0fIc",
        "outputId": "56aee99c-b33c-4a82-b590-0f066d7fb96b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  `dkt.py` source code - 수정\n",
        "\n"
      ],
      "metadata": {
        "id": "3T3FI2rz09Xg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "#   Deep Knowledge Tracing (DKT) Implementation\n",
        "#   Mohammad M H Khajah <mohammad.khajah@colorado.edu>\n",
        "#   Copyright (c) 2016 all rights reserved.\n",
        "#\n",
        "#   How to use:\n",
        "#       python dkt.py dataset.txt dataset_split.txt\n",
        "#\n",
        "#   Script saves 3 files:\n",
        "#       dataset.txt.model_weights trained model weights\n",
        "#       dataset.txt.history training history (training LL, test AUC)\n",
        "#       dataset.txt.preds predictions for test trials\n",
        "#\n",
        "import os\n",
        "import sys\n",
        "from keras.src.layers import SpectralNormalization\n",
        "from keras.src.utils import split_dataset\n",
        "import numpy as np\n",
        "from keras.preprocessing import sequence\n",
        "#from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import TimeDistributed, Masking, Dense\n",
        "from keras.layers import LSTM\n",
        "from keras import backend as K\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import keras.layers\n",
        "import theano.tensor as Th\n",
        "import random\n",
        "import math\n",
        "import argparse\n",
        "import tensorflow as tf\n",
        "from keras import backend as K\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Process some integers.')\n",
        "    parser.add_argument('--dataset', type=str, help='Dataset file', required=True)\n",
        "    #parser.add_argument('--splitfile', type=str, help='Split file', required=True)\n",
        "    parser.add_argument('--hiddenunits', type=int, help='Number of LSTM hidden units.',\n",
        "                        default=200, required=False)\n",
        "    parser.add_argument('--batchsize', type=int, help='Number of sequences to process in a batch.',\n",
        "                        default=20, required=False)\n",
        "    parser.add_argument('--timewindow', type=int, help='Number of timesteps to process in a batch.',\n",
        "                        default=100, required=False)\n",
        "    parser.add_argument('--epochs', type=int, help='Number of epochs.',\n",
        "                        default=50, required=False)\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    dataset = args.dataset\n",
        "    split_file = args.splitfile\n",
        "    hidden_units = args.hiddenunits\n",
        "    batch_size = args.batchsize\n",
        "    time_window = args.timewindow\n",
        "    epochs = args.epochs\n",
        "\n",
        "    model_file = '2' + dataset + '.model_weights'\n",
        "    history_file = '2' + dataset + '.history'\n",
        "    preds_file = '2' + dataset + '.preds'\n",
        "\n",
        "    overall_loss = [0.0]\n",
        "    preds = []\n",
        "    history = []\n",
        "\n",
        "    # load dataset\n",
        "    training_seqs, testing_seqs, num_skills = load_dataset(dataset)\n",
        "    print (\"Training Sequences: %d\" % len(training_seqs))\n",
        "    print (\"Testing Sequences: %d\" % len(testing_seqs))\n",
        "    print (\"Number of skills: %d\" % num_skills)\n",
        "\n",
        "    # Our loss function\n",
        "    # The model gives predictions for all skills so we need to get the\n",
        "    # prediction for the skill at time t. We do that by taking the column-wise\n",
        "    # dot product between the predictions at each time slice and a\n",
        "    # one-hot encoding of the skill at time t.\n",
        "    # y_true: (nsamples x nsteps x nskills+1)\n",
        "    # y_pred: (nsamples x nsteps x nskills)\n",
        "    def loss_function(y_true, y_pred):\n",
        "\n",
        "        skill = y_true[:,:,0:num_skills]\n",
        "        obs = y_true[:,:,num_skills]\n",
        "        otro = tf.dtypes.cast(skill, tf.float32)\n",
        "        multiplica = tf.math.multiply(y_pred , otro)\n",
        "        rel_pred = tf.reduce_sum(y_pred * skill, axis=2)\n",
        "\n",
        "        # keras implementation does a mean on the last dimension (axis=-1) which\n",
        "        # it assumes is a singleton dimension. But in our context that would\n",
        "        # be wrong.\n",
        "        return K.binary_crossentropy(rel_pred, obs)\n",
        "\n",
        "    # build model\n",
        "    model = Sequential()\n",
        "\n",
        "    # ignore padding\n",
        "    model.add(Masking(-1.0, batch_input_shape=(batch_size, time_window, num_skills*2)))\n",
        "\n",
        "    # lstm configured to keep states between batches\n",
        "    model.add(LSTM(input_dim = num_skills*2,\n",
        "                   units = hidden_units,\n",
        "                   return_sequences=True,\n",
        "                   batch_input_shape=(batch_size, time_window, num_skills*2),\n",
        "                   stateful = True\n",
        "    ))\n",
        "\n",
        "    # readout layer. TimeDistributedDense uses the same weights for all\n",
        "    # time steps.\n",
        "    model.add(TimeDistributed(Dense(input_dim = hidden_units,\n",
        "        units = num_skills, activation='sigmoid')))\n",
        "\n",
        "    # optimize with rmsprop which dynamically adapts the learning\n",
        "    # rate of each weight.\n",
        "    model.compile(loss=loss_function,\n",
        "                optimizer='rmsprop') #,class_mode=\"binary\"\n",
        "\n",
        "    # model load weights!\n",
        "    model.load_weights(model_file)\n",
        "\n",
        "    # training function\n",
        "    def trainer(X, Y):\n",
        "        batch_loss = model.train_on_batch(X, Y)\n",
        "        overall_loss[0] += batch_loss\n",
        "\n",
        "    # prediction\n",
        "    def predictor(X, Y):\n",
        "        batch_activations = model.predict_on_batch(X)\n",
        "        skill = Y[:,:,0:num_skills]\n",
        "        obs = Y[:,:,num_skills]\n",
        "        y_pred = np.squeeze(np.array(batch_activations))\n",
        "\n",
        "        rel_pred = np.sum(y_pred * skill, axis=2)\n",
        "\n",
        "        for b in range(0, X.shape[0]):\n",
        "            for t in range(0, X.shape[1]):\n",
        "                if X[b, t, 0] == -1.0:\n",
        "                    continue\n",
        "                preds.append((rel_pred[b][t], obs[b][t]))\n",
        "\n",
        "    # call when prediction batch is finished\n",
        "    # resets LSTM state because we are done with all sequences in the batch\n",
        "    def finished_prediction_batch(percent_done):\n",
        "        model.reset_states()\n",
        "\n",
        "    # similiar to the above\n",
        "    def finished_batch(percent_done):\n",
        "        print (\"(%4.3f %%) %f\" % (percent_done, overall_loss[0]))\n",
        "        model.reset_states()\n",
        "\n",
        "    # run the model\n",
        "    for e in range(0, epochs):\n",
        "        model.reset_states()\n",
        "\n",
        "        # train\n",
        "        run_func(training_seqs, num_skills, trainer, batch_size, time_window, finished_batch)\n",
        "\n",
        "        model.reset_states()\n",
        "\n",
        "        # test\n",
        "        run_func(testing_seqs, num_skills, predictor, batch_size, time_window, finished_prediction_batch)\n",
        "\n",
        "        # compute AUC\n",
        "        y1 = [p[1] for p in preds] #y_true\n",
        "        y2 = [p[0] for p in preds] #y_score (pred)\n",
        "        auc = roc_auc_score(y1, y2)\n",
        "\n",
        "        # log\n",
        "        history.append((overall_loss[0], auc))\n",
        "\n",
        "        # save model\n",
        "        model.save_weights(model_file, overwrite=True)\n",
        "        print (\"==== Epoch: %d, Test AUC: %f\" % (e, auc))\n",
        "\n",
        "        # reset loss\n",
        "        overall_loss[0] = 0.0\n",
        "\n",
        "        # save predictions\n",
        "        with open(preds_file, 'w') as f:\n",
        "            f.write('was_heldout\\tprob_recall\\tstudent_recalled\\n')\n",
        "            for pred in preds:\n",
        "                f.write('1\\t%f\\t%d\\n' % (pred[0], pred[1]))\n",
        "\n",
        "        with open(history_file, 'w') as f:\n",
        "            for h in history:\n",
        "                f.write('\\t'.join([str(he) for he in h]))\n",
        "                f.write('\\n')\n",
        "\n",
        "        # clear preds\n",
        "        preds = []\n",
        "\n",
        "\n",
        "\n",
        "# 모델 학습, 평가하는 함수\n",
        "# seqs: interactions (skill(KC), is_correct) 문제와 정답여부 쌍\n",
        "# num_skills: # of KC\n",
        "# f: training or test fuunc\n",
        "def run_func(seqs, num_skills, f, batch_size, time_window, batch_done = None):\n",
        "\n",
        "    assert(min([len(s) for s in seqs]) > 0)\n",
        "\n",
        "    # randomize samples\n",
        "    seqs = seqs[:]\n",
        "    random.shuffle(seqs)\n",
        "\n",
        "    processed = 0\n",
        "    for start_from in range(0, len(seqs), batch_size):\n",
        "       end_before = min(len(seqs), start_from + batch_size)\n",
        "       x = []\n",
        "       y = []\n",
        "       for seq in seqs[start_from:end_before]:\n",
        "           x_seq = []\n",
        "           y_seq = []\n",
        "           xt_zeros = [0 for i in range(0, num_skills*2)]\n",
        "           ct_zeros = [0 for i in range(0, num_skills+1)]\n",
        "           xt = xt_zeros[:]\n",
        "\n",
        "           for skill, is_correct in seq:\n",
        "               x_seq.append(xt)\n",
        "\n",
        "               ct = ct_zeros[:]\n",
        "               ct[skill] = 1\n",
        "               ct[num_skills] = is_correct\n",
        "               y_seq.append(ct)\n",
        "\n",
        "               # one hot encoding of (last_skill, is_correct)\n",
        "               # 앞에서부터 각각 1번 문제 틀린 경우, 1번 문제 맞은 경우, 2번 문제 틀린 경우, 2번 문제 맞은 경우, ... 를 나타냄.\n",
        "               pos = skill * 2 + is_correct\n",
        "               xt = xt_zeros[:]\n",
        "               xt[pos] = 1\n",
        "\n",
        "           x.append(x_seq)\n",
        "           y.append(y_seq)\n",
        "\n",
        "       maxlen = max([len(s) for s in x])\n",
        "       maxlen = round_to_multiple(maxlen, time_window)\n",
        "       # fill up the batch if necessary\n",
        "       if len(x) < batch_size:\n",
        "            for e in range(0, batch_size - len(x)):\n",
        "                x_seq = []\n",
        "                y_seq = []\n",
        "                for t in range(0, time_window):\n",
        "                    x_seq.append([-1.0 for i in range(0, num_skills*2)])\n",
        "                    y_seq.append([0.0 for i in range(0, num_skills+1)])\n",
        "                x.append(x_seq)\n",
        "                y.append(y_seq)\n",
        "\n",
        "       X = pad_sequences(x, padding='post', maxlen = maxlen, dim=num_skills*2, value=-1.0)\n",
        "       Y = pad_sequences(y, padding='post', maxlen = maxlen, dim=num_skills+1, value=-1.0)\n",
        "\n",
        "       for t in range(0, maxlen, time_window):\n",
        "           f(X[:,t:(t+time_window),:], Y[:,t:(t+time_window),:])\n",
        "\n",
        "       processed += end_before - start_from\n",
        "\n",
        "       # reset the states for the next batch of sequences\n",
        "       if batch_done:\n",
        "           batch_done((processed * 100.0) / len(seqs))\n",
        "\n",
        "def round_to_multiple(x, base):\n",
        "    return int(base * math.ceil(float(x)/base))\n",
        "\n",
        "def load_dataset(dataset):\n",
        "    seqs, num_skills = read_file(dataset)\n",
        "\n",
        "    # with open(split_file, 'r') as f:\n",
        "    #     student_assignment = f.read().split(' ')\n",
        "    #     print(len(student_assignment))\n",
        "    #     print(student_assignment.index('0'))\n",
        "\n",
        "    # training_seqs = [seqs[i] for i in range(0, len(seqs)) if student_assignment[i] == '1']\n",
        "    # testing_seqs = [seqs[i] for i in range(0, len(seqs)) if student_assignment[i] == '0']\n",
        "\n",
        "    # training:test = 9:1 비율로 나눠줌. split file 사용 안하고 인덱스 기준으로 슬라이스 함.\n",
        "    training_seqs = seqs[:1031]\n",
        "    testing_seqs = seqs[1031:]\n",
        "\n",
        "    return training_seqs, testing_seqs, num_skills\n",
        "\n",
        "def read_file(dataset_path):\n",
        "    seqs_by_student = {}\n",
        "    problem_ids = {}\n",
        "    next_problem_id = 0\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        for line in f:\n",
        "            student, problem, is_correct = line.split(' ')\n",
        "            student = int(student)\n",
        "            if student not in seqs_by_student:\n",
        "                seqs_by_student[student] = []\n",
        "            if problem not in problem_ids:\n",
        "                problem_ids[problem] = next_problem_id\n",
        "                next_problem_id += 1\n",
        "            seqs_by_student[student].append((problem_ids[problem], int(is_correct == \"1\\n\")))\n",
        "\n",
        "    sorted_keys = sorted(seqs_by_student.keys())\n",
        "    return [seqs_by_student[k] for k in sorted_keys], next_problem_id\n",
        "\n",
        "# https://groups.google.com/forum/#!msg/keras-users/7sw0kvhDqCw/QmDMX952tq8J\n",
        "def pad_sequences(sequences, maxlen=None, dim=1, dtype='float32',\n",
        "    padding='pre', truncating='pre', value=0.):\n",
        "    '''\n",
        "        Override keras method to allow multiple feature dimensions.\n",
        "\n",
        "        @dim: input feature dimension (number of features per timestep)\n",
        "    '''\n",
        "    lengths = [len(s) for s in sequences]\n",
        "\n",
        "    nb_samples = len(sequences)\n",
        "    if maxlen is None:\n",
        "        maxlen = np.max(lengths)\n",
        "\n",
        "    x = (np.ones((nb_samples, maxlen, dim)) * value).astype(dtype)\n",
        "    for idx, s in enumerate(sequences):\n",
        "        if truncating == 'pre':\n",
        "            trunc = s[-maxlen:]\n",
        "        elif truncating == 'post':\n",
        "            trunc = s[:maxlen]\n",
        "        else:\n",
        "            raise ValueError(\"Truncating type '%s' not understood\" % padding)\n",
        "\n",
        "        if padding == 'post':\n",
        "            x[idx, :len(trunc)] = trunc\n",
        "        elif padding == 'pre':\n",
        "            x[idx, -len(trunc):] = trunc\n",
        "        else:\n",
        "            raise ValueError(\"Padding type '%s' not understood\" % padding)\n",
        "    return x\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "JKE17zVt0y44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## run"
      ],
      "metadata": {
        "id": "iHJLBa3h1gsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/SQuiz_dh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZtivirK1gG9",
        "outputId": "3ecd0397-a5ab-4ed7-bf6c-5e9c8d817eab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1Q9M0zm-YgscnlwwNl0JoXWQewvHIZLk8/SQuiz_dh\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install theano"
      ],
      "metadata": {
        "id": "R_z95g6o8OZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall numpy"
      ],
      "metadata": {
        "id": "2AKYRdKH83LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install numpy==1.23.1"
      ],
      "metadata": {
        "id": "NfTIyWuO895U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/use/local/python3.10/.... /theano/configdefault.py\n",
        "\n",
        "line 1284\n",
        "\n",
        "blas_info = np.distutils.__config__.blas_ilp64_opt_info\n",
        "\n",
        "로 수정\n"
      ],
      "metadata": {
        "id": "kiAQu6DFqXEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!THEANO_FLAGS=\"device=cuda*,floatX=float32\" python dkt.py --dataset dataset.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NnmzZ0Lr_ohg",
        "outputId": "ff1b80c7-43f1-44be-bf97-e94772f16e6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-27 16:22:02.995830: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-27 16:22:02.995885: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-27 16:22:02.997213: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-27 16:22:03.004535: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-04-27 16:22:04.027389: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "ERROR (theano.gpuarray): pygpu was configured but could not be imported or is too old (version 0.7 or higher required)\n",
            "NoneType: None\n",
            "3686872\n",
            "3679199\n",
            "Training Sequences: 1031\n",
            "Testing Sequences: 115\n",
            "Number of skills: 465\n",
            "2024-04-27 16:22:12.798984: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-27 16:22:13.391296: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-27 16:22:13.391596: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-27 16:22:13.392356: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-27 16:22:13.392617: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-27 16:22:13.392855: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-27 16:22:13.625195: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-27 16:22:13.625506: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-27 16:22:13.625649: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:47] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2024-04-27 16:22:13.625769: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2024-04-27 16:22:13.625927: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13949 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "2024-04-27 16:22:26.838873: W tensorflow/core/common_runtime/type_inference.cc:339] Type inference failed. This indicates an invalid graph that escaped type checking. Error message: INVALID_ARGUMENT: expected compatible input types, but input 1:\n",
            "type_id: TFT_OPTIONAL\n",
            "args {\n",
            "  type_id: TFT_PRODUCT\n",
            "  args {\n",
            "    type_id: TFT_TENSOR\n",
            "    args {\n",
            "      type_id: TFT_INT32\n",
            "    }\n",
            "  }\n",
            "}\n",
            " is neither a subtype nor a supertype of the combined inputs preceding it:\n",
            "type_id: TFT_OPTIONAL\n",
            "args {\n",
            "  type_id: TFT_PRODUCT\n",
            "  args {\n",
            "    type_id: TFT_TENSOR\n",
            "    args {\n",
            "      type_id: TFT_FLOAT\n",
            "    }\n",
            "  }\n",
            "}\n",
            "\n",
            "\tfor Tuple type infernce function 0\n",
            "\twhile inferring type of node 'cond_19/output/_22'\n",
            "2024-04-27 16:22:28.547937: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8906\n",
            "2024-04-27 16:22:28.916587: I external/local_xla/xla/service/service.cc:168] XLA service 0x7be8f0c6c0f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2024-04-27 16:22:28.916634: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2024-04-27 16:22:28.937602: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1714234949.045003    1951 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
            "(1.940 %) 112.289166\n",
            "(3.880 %) 238.607059\n",
            "(5.820 %) 349.441116\n",
            "(7.759 %) 439.262767\n",
            "(9.699 %) 564.473664\n",
            "(11.639 %) 800.057184\n",
            "(13.579 %) 894.648887\n",
            "(15.519 %) 1107.684252\n",
            "(17.459 %) 1284.363605\n",
            "(19.399 %) 1367.200689\n",
            "(21.339 %) 1460.672185\n",
            "(23.278 %) 1652.049528\n",
            "(25.218 %) 1827.072550\n",
            "(27.158 %) 1924.880070\n",
            "(29.098 %) 2023.849157\n",
            "(31.038 %) 2225.749162\n",
            "(32.978 %) 2320.343718\n",
            "(34.918 %) 2463.365101\n",
            "(36.857 %) 2600.171588\n",
            "(38.797 %) 2725.465956\n",
            "(40.737 %) 2799.439518\n",
            "(42.677 %) 2981.947615\n",
            "(44.617 %) 3086.021060\n",
            "(46.557 %) 3266.542271\n",
            "(48.497 %) 3375.556729\n",
            "(50.436 %) 3483.485649\n",
            "(52.376 %) 3617.170043\n",
            "(54.316 %) 3690.207290\n",
            "(56.256 %) 3771.002178\n",
            "(58.196 %) 3880.378504\n",
            "(60.136 %) 3986.957443\n",
            "(62.076 %) 4069.420866\n",
            "(64.016 %) 4219.158276\n",
            "(65.955 %) 4341.980036\n",
            "(67.895 %) 4458.738762\n",
            "(69.835 %) 4636.263749\n",
            "(71.775 %) 4746.777964\n",
            "(73.715 %) 4893.924824\n",
            "(75.655 %) 5024.063760\n",
            "(77.595 %) 5150.966643\n",
            "(79.534 %) 5240.613343\n",
            "(81.474 %) 5391.959986\n",
            "(83.414 %) 5703.379338\n",
            "(85.354 %) 5847.197778\n",
            "(87.294 %) 6038.636014\n",
            "(89.234 %) 6162.295028\n",
            "(91.174 %) 6265.831379\n",
            "(93.113 %) 6369.964609\n",
            "(95.053 %) 6478.231409\n",
            "(96.993 %) 6711.718901\n",
            "(98.933 %) 6839.316396\n",
            "(100.000 %) 6901.923736\n",
            "==== Epoch: 47, Test AUC: 0.792854\n",
            "(1.940 %) 115.166956\n",
            "(3.880 %) 236.988722\n",
            "(5.820 %) 369.520691\n",
            "(7.759 %) 527.734514\n",
            "(9.699 %) 668.639005\n",
            "(11.639 %) 744.154852\n",
            "(13.579 %) 838.013324\n",
            "(15.519 %) 906.786196\n",
            "(17.459 %) 1093.225485\n",
            "(19.399 %) 1209.584699\n",
            "(21.339 %) 1349.124350\n",
            "(23.278 %) 1434.906119\n",
            "(25.218 %) 1568.317452\n",
            "(27.158 %) 1726.479969\n",
            "(29.098 %) 1836.718099\n",
            "(31.038 %) 2141.918747\n",
            "(32.978 %) 2256.064446\n",
            "(34.918 %) 2387.507370\n",
            "(36.857 %) 2499.706465\n",
            "(38.797 %) 2574.812464\n",
            "(40.737 %) 2773.170187\n",
            "(42.677 %) 2855.645750\n",
            "(44.617 %) 2973.418126\n",
            "(46.557 %) 3088.718396\n",
            "(48.497 %) 3252.434938\n",
            "(50.436 %) 3349.354002\n",
            "(52.376 %) 3478.023437\n",
            "(54.316 %) 3581.642123\n",
            "(56.256 %) 3669.722722\n",
            "(58.196 %) 3748.214549\n",
            "(60.136 %) 3883.957583\n",
            "(62.076 %) 4024.783730\n",
            "(64.016 %) 4108.088438\n",
            "(65.955 %) 4196.053485\n",
            "(67.895 %) 4424.690936\n",
            "(69.835 %) 4551.964858\n",
            "(71.775 %) 4754.225074\n",
            "(73.715 %) 4853.912033\n",
            "(75.655 %) 4938.265910\n",
            "(77.595 %) 5064.081808\n",
            "(79.534 %) 5156.715436\n",
            "(81.474 %) 5334.402468\n",
            "(83.414 %) 5453.702569\n",
            "(85.354 %) 5628.996292\n",
            "(87.294 %) 5698.311110\n",
            "(89.234 %) 5835.378897\n",
            "(91.174 %) 5976.202910\n",
            "(93.113 %) 6109.897558\n",
            "(95.053 %) 6256.394263\n",
            "(96.993 %) 6439.747017\n",
            "(98.933 %) 6540.962651\n",
            "(100.000 %) 6721.605243\n",
            "==== Epoch: 48, Test AUC: 0.793311\n",
            "(1.940 %) 184.589961\n",
            "(3.880 %) 305.452365\n",
            "(5.820 %) 478.407849\n",
            "(7.759 %) 579.672455\n",
            "(9.699 %) 691.972798\n",
            "(11.639 %) 822.699092\n",
            "(13.579 %) 941.868682\n",
            "(15.519 %) 1100.866562\n",
            "(17.459 %) 1249.826219\n",
            "(19.399 %) 1370.739062\n",
            "(21.339 %) 1459.004777\n",
            "(23.278 %) 1589.418688\n",
            "(25.218 %) 1652.585665\n",
            "(27.158 %) 1781.218378\n",
            "(29.098 %) 1851.948569\n",
            "(31.038 %) 1983.249867\n",
            "(32.978 %) 2219.588339\n",
            "(34.918 %) 2294.750860\n",
            "(36.857 %) 2494.130675\n",
            "(38.797 %) 2576.029544\n",
            "(40.737 %) 2676.161799\n",
            "(42.677 %) 2768.480199\n",
            "(44.617 %) 2910.811947\n",
            "(46.557 %) 3005.556797\n",
            "(48.497 %) 3130.240587\n",
            "(50.436 %) 3205.789996\n",
            "(52.376 %) 3385.784320\n",
            "(54.316 %) 3539.490630\n",
            "(56.256 %) 3617.362574\n",
            "(58.196 %) 3705.443151\n",
            "(60.136 %) 3902.869744\n",
            "(62.076 %) 4004.877627\n",
            "(64.016 %) 4159.718230\n",
            "(65.955 %) 4251.439689\n",
            "(67.895 %) 4358.535902\n",
            "(69.835 %) 4433.323064\n",
            "(71.775 %) 4633.212991\n",
            "(73.715 %) 4710.986105\n",
            "(75.655 %) 4825.640466\n",
            "(77.595 %) 4949.469785\n",
            "(79.534 %) 5027.705189\n",
            "(81.474 %) 5145.858850\n",
            "(83.414 %) 5208.756458\n",
            "(85.354 %) 5342.899904\n",
            "(87.294 %) 5479.645775\n",
            "(89.234 %) 5548.958095\n",
            "(91.174 %) 5723.828596\n",
            "(93.113 %) 6034.836698\n",
            "(95.053 %) 6135.637544\n",
            "(96.993 %) 6238.272024\n",
            "(98.933 %) 6384.577065\n",
            "(100.000 %) 6499.860178\n",
            "==== Epoch: 49, Test AUC: 0.793725\n",
            "(1.940 %) 199.156199\n",
            "(3.880 %) 319.234044\n",
            "(5.820 %) 413.849912\n",
            "(7.759 %) 503.467656\n",
            "(9.699 %) 582.987630\n",
            "(11.639 %) 715.000853\n",
            "(13.579 %) 874.051029\n",
            "(15.519 %) 981.973097\n",
            "(17.459 %) 1055.865140\n",
            "(19.399 %) 1165.164690\n",
            "(21.339 %) 1274.122090\n",
            "(23.278 %) 1351.571967\n",
            "(25.218 %) 1483.587490\n",
            "(27.158 %) 1777.459472\n",
            "(29.098 %) 1922.671191\n",
            "(31.038 %) 2021.113158\n",
            "(32.978 %) 2115.195088\n",
            "(34.918 %) 2214.741722\n",
            "(36.857 %) 2317.841950\n",
            "(38.797 %) 2428.800053\n",
            "(40.737 %) 2600.443857\n",
            "(42.677 %) 2676.145266\n",
            "(44.617 %) 2781.609694\n",
            "(46.557 %) 2987.259736\n",
            "(48.497 %) 3160.790875\n",
            "(50.436 %) 3252.945997\n",
            "(52.376 %) 3352.156435\n",
            "(54.316 %) 3569.584904\n",
            "(56.256 %) 3773.146970\n",
            "(58.196 %) 3968.873942\n",
            "(60.136 %) 4049.170301\n",
            "(62.076 %) 4190.916176\n",
            "(64.016 %) 4286.439762\n",
            "(65.955 %) 4421.023180\n",
            "(67.895 %) 4529.045552\n",
            "(69.835 %) 4678.897707\n",
            "(71.775 %) 4846.586848\n",
            "(73.715 %) 4912.716579\n",
            "(75.655 %) 5031.458760\n",
            "(77.595 %) 5170.226203\n",
            "(79.534 %) 5292.786223\n",
            "(81.474 %) 5498.281971\n",
            "(83.414 %) 5567.071964\n",
            "(85.354 %) 5682.052056\n",
            "(87.294 %) 5827.598922\n",
            "(89.234 %) 5946.830380\n",
            "(91.174 %) 6041.550731\n",
            "(93.113 %) 6236.489518\n",
            "(95.053 %) 6311.417148\n",
            "(96.993 %) 6410.558736\n",
            "(98.933 %) 6486.656466\n",
            "(100.000 %) 6568.589417\n",
            "==== Epoch: 50, Test AUC: 0.784163\n"
          ]
        }
      ]
    }
  ]
}